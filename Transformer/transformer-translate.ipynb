{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "device = 'cuda'\n",
    "epochs = 100\n",
    "\n",
    "src_len = 8  # 源句子最大长度 encoder_input max seq len\n",
    "tgt_len = 7  # decoder_input(=output) max seq len\n",
    "\n",
    "# transformer网络参数\n",
    "d_model = 512  # Embedding Size (token embedding 和 position embedding 的编码维度)\n",
    "d_ff = 2048  # Feed Farward Dimension (512->2048->512)\n",
    "d_k = d_v = 64  # Dimension of K(=Q) and V (K和Q的维度是相同的，这里为了方便让V也等于Q)\n",
    "n_layers = 6  # Number of encoder and decoder layer blocks\n",
    "n_heads = 8  # Number of heads in Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集\n",
    "\n",
    "# 训练集\n",
    "sentences = [\n",
    "    # 中文和英语的单词个数不要求相同\n",
    "    # encoder_input           decoder_input               decoder_output\n",
    "    ['我 有 一 个 好 朋 友 P', 'S i have a good friend .', 'i have a good friend . E'],\n",
    "    ['我 有 零 个 女 朋 友 P', 'S i have zero girl friend .', 'i have zero girl friend . E']\n",
    "]\n",
    "\n",
    "# 测试集\n",
    "# 输入：”我 有 一 个 女 朋 友“\n",
    "# 输出：”i have a girlfriend“\n",
    "\n",
    "# 构建中文词表\n",
    "src_vocab = {'P': 0, '我': 1, '有': 2, '一': 3,\n",
    "             '个': 4, '好': 5, '朋': 6, '友': 7, '零': 8, '女': 9}\n",
    "src_idx2word = {i: w for i, w in enumerate(src_vocab)}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "# 构建英文词表\n",
    "tgt_vocab = {'P': 0, 'i': 1, 'have': 2, 'a': 3, 'good': 4,\n",
    "             'friend': 5, 'zero': 6, 'girl': 7, 'S': 8, 'E': 9, '.': 10}\n",
    "tgt_idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "# 构建数据\n",
    "\n",
    "\n",
    "def make_data(sentences):\n",
    "    \"\"\"把句子中的单词序列转换为单词对应词表中的索引序列\"\"\"\n",
    "    encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "        encoder_input = [[src_vocab[n] for n in sentences[i][0].split()]]\n",
    "        decoder_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]\n",
    "        decoder_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]\n",
    "\n",
    "        encoder_inputs.extend(encoder_input)\n",
    "        decoder_inputs.extend(decoder_input)\n",
    "        decoder_outputs.extend(decoder_output)\n",
    "\n",
    "    return torch.LongTensor(encoder_inputs), torch.LongTensor(decoder_inputs), torch.LongTensor(decoder_outputs)\n",
    "\n",
    "\n",
    "encoder_inputs, decoder_inputs, decoder_outputs = make_data(sentences)\n",
    "# encoder_inputs.shape[0],encoder_inputs,decoder_inputs,decoder_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataSet(Data.Dataset):\n",
    "    \"\"\"自定义DataSet\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_inputs, decoder_inputs, decoder_outputs):\n",
    "        super(SentenceDataSet, self).__init__()\n",
    "        self.encoder_inputs = encoder_inputs\n",
    "        self.decoder_inputs = decoder_inputs\n",
    "        self.decoder_outputs = decoder_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encoder_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoder_inputs[idx], self.decoder_inputs[idx], self.decoder_outputs[idx]\n",
    "\n",
    "\n",
    "loader = Data.DataLoader(SentenceDataSet(\n",
    "    encoder_inputs, decoder_inputs, decoder_outputs), 2, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        i_mat = torch.pow(10000, torch.arange(0, d_model, 2) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position / i_mat)\n",
    "        pe[:, 1::2] = torch.cos(position / i_mat)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)  # 在模型中定义一个常量pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x:[seq_len, batch_size, d_model]\"\"\"\n",
    "        x = x+self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_padding_mask(seq):\n",
    "    \"\"\"在对value向量加权和的时候, 使得padding的位置的alpha_ij=0\"\"\"\n",
    "    # seq:[batch_size, seq_len]\n",
    "\n",
    "    batch_size, seq_len = seq.size()\n",
    "    padding_attention_mask = seq.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "\n",
    "    # [batch_size, seq_len, seq_len]\n",
    "    return padding_attention_mask.expand(batch_size, seq_len, seq_len)\n",
    "\n",
    "# seq_k = torch.Tensor([[1,2,3,4,0], [1,0,3,5,0]])\n",
    "# print(get_attention_padding_mask(seq_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Subsequence Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attention_subsequence_mask(seq):\n",
    "    \"\"\"用于decoder_input的上三角mask\"\"\"\n",
    "    # seq:[batch_size, tgt_len]\n",
    "\n",
    "    # attn_shape:[batch_size, tgt_len, tgt_len]\n",
    "    attention_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "\n",
    "    # 返回一个上三角矩阵，第k条对角线以下元素全为0（主对角线为第0条）\n",
    "    subsequence_mask = np.triu(np.ones(attention_shape), k=1)\n",
    "\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "\n",
    "    return subsequence_mask\n",
    "\n",
    "seq_k = torch.Tensor([[1,2,3,4,0], [1,0,3,5,0]])\n",
    "get_attention_subsequence_mask(seq_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        Q: [batch_size, n_heads, len_q, d_k]\n",
    "        K: [batch_size, n_heads, len_k, d_k]\n",
    "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        说明: 在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同\n",
    "        \"\"\"\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2) / np.sqrt(d_k)) # scores:[batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        # 使用mask矩阵填充scores(将scores中对应attn_mask为True的位置变为-1e9)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "\n",
    "        attn = nn.Softmax(dim=-1)(scores) \n",
    "\n",
    "        # attn:[batch_size, n_heads, len_q, len_k]\n",
    "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
    "        context = torch.matmul(attn, V) # context: [batch_size, n_heads, len_q, d_v]\n",
    "        \n",
    "        # context：[[z1,z2,...],[...]]向量, attn为注意力稀疏矩阵（用于可视化的）\n",
    "        return context, attn\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a0e993b1d7c23b73f928b0a34114fba5233a538b73c46f7e69aa6023163c42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
