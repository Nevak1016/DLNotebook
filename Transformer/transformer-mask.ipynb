{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3249,  1.4673, -0.5678,  0.4758, -0.1507, -0.7540,  1.1388, -0.1609],\n",
      "        [ 0.1754,  0.2556,  1.3769,  1.4690, -0.3868,  0.2991,  1.0700, -1.7144],\n",
      "        [-0.2577, -0.0318,  0.3799, -1.4091, -1.4868,  1.7403, -0.2968,  2.6475],\n",
      "        [ 0.2403, -0.2809, -0.1320,  1.0894,  0.3982, -0.0417, -0.1605,  1.3249],\n",
      "        [-1.5856,  0.0556, -1.1777,  1.2505,  1.0750,  0.8235,  1.6107, -1.5065],\n",
      "        [ 0.1568, -0.1742,  0.2178, -1.0936, -0.5422, -2.2246,  0.1710, -1.3192],\n",
      "        [ 0.5250,  0.1284,  1.4986,  0.7969, -0.0632,  0.5656, -1.4087, -0.3530],\n",
      "        [ 0.1960,  0.1431, -1.0588, -1.2810, -1.4329,  0.5558,  1.8252,  0.8264],\n",
      "        [-0.7362, -0.6730,  1.0818,  1.5130,  1.8261, -0.6710,  0.9524,  1.7513]],\n",
      "       requires_grad=True)\n",
      "tensor([[3, 7, 0, 0],\n",
      "        [1, 4, 6, 7]])\n",
      "tensor([[[ 0.2403, -0.2809, -0.1320,  1.0894,  0.3982, -0.0417, -0.1605,\n",
      "           1.3249],\n",
      "         [ 0.1960,  0.1431, -1.0588, -1.2810, -1.4329,  0.5558,  1.8252,\n",
      "           0.8264],\n",
      "         [-0.3249,  1.4673, -0.5678,  0.4758, -0.1507, -0.7540,  1.1388,\n",
      "          -0.1609],\n",
      "         [-0.3249,  1.4673, -0.5678,  0.4758, -0.1507, -0.7540,  1.1388,\n",
      "          -0.1609]],\n",
      "\n",
      "        [[ 0.1754,  0.2556,  1.3769,  1.4690, -0.3868,  0.2991,  1.0700,\n",
      "          -1.7144],\n",
      "         [-1.5856,  0.0556, -1.1777,  1.2505,  1.0750,  0.8235,  1.6107,\n",
      "          -1.5065],\n",
      "         [ 0.5250,  0.1284,  1.4986,  0.7969, -0.0632,  0.5656, -1.4087,\n",
      "          -0.3530],\n",
      "         [ 0.1960,  0.1431, -1.0588, -1.2810, -1.4329,  0.5558,  1.8252,\n",
      "           0.8264]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# word embedding（序列模型）\n",
    "# source sentence 和 target sentence\n",
    "\n",
    "# 构建序列，以词表中的索引表示序列的字符\n",
    "batch_size = 2\n",
    "# 单词表的大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# embedding后的维度，原论文中的512\n",
    "model_dim = 8\n",
    "\n",
    "# 序列最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5 \n",
    "max_position_len = 5\n",
    " \n",
    "# src_len = torch.randint(2, 5, (batch_size,))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "\n",
    "# 以单词索引构成句子\n",
    "src_seq = [torch.randint(1, max_num_src_words, (L,)) for L in src_len]\n",
    "# 构建batch，seq需要padding成同样的长度，默认补0\n",
    "src_seq = [F.pad(seq, (0, max(src_len)-len(seq))) for seq in src_seq]\n",
    "# 将每个seq变成二维，以便拼接成二位张量batch输入\n",
    "src_seq = [torch.unsqueeze(seq, 0) for seq in src_seq]\n",
    "src_seq = torch.cat(src_seq)\n",
    "\n",
    "# tgt数据同样操作，这里简写\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max(tgt_len)-L)), 0) for L in tgt_len])\n",
    "\n",
    "# 构造embedding\n",
    "# nn.Embedding用于初始化一个weight table，实际句子里的word根据自己的索引去取自己的weight\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "print(src_embedding_table.weight)\n",
    "print(src_seq)\n",
    "print(src_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# position embedding\n",
    "\n",
    "# 表示pos，从0到model_dim\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1)\n",
    "# 表示10000^(2i/d_model)\n",
    "i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape(1, -1) / model_dim)\n",
    "\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "\n",
    "# pos_mat会自动广播\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat/i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat/i_mat)\n",
    "# print(pe_embedding_table)\n",
    "\n",
    "# 使用pe_embedding_table构造nn.Embedding，借用其forward方法获取word对应的pe\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad = False)\n",
    "\n",
    "# 传入pe_embedding的是word在句子中的位置索引\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1886, 0.1609, 0.2342, 0.1948, 0.2215]),\n",
       " tensor([3.8058e-10, 4.8409e-17, 9.9631e-01, 9.8580e-09, 3.6889e-03]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax demo, why Scaled?\n",
    "alpha1 = 0.1\n",
    "alpha2 = 10\n",
    "score = torch.randn(5)\n",
    "prob1 = F.softmax(score*alpha1, -1)\n",
    "prob2 = F.softmax(score*alpha2, -1)\n",
    "prob1, prob2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([[[-0.4269,  0.7850,  1.6566, -1.0907],\n",
      "         [ 0.9207, -0.3187,  1.4922,  2.3232],\n",
      "         [ 1.6359,  0.7555,  0.7687, -0.7684],\n",
      "         [-0.2459,  1.2063,  0.0726, -0.9182]],\n",
      "\n",
      "        [[ 0.8725,  0.4440, -0.7807,  0.7333],\n",
      "         [ 1.8880, -0.1158,  0.1617,  0.7443],\n",
      "         [ 0.6607,  0.9006, -0.1254, -1.0446],\n",
      "         [ 0.6347,  0.7117, -0.0414,  1.0391]]])\n",
      "tensor([[[-4.2693e-01,  7.8505e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [ 9.2069e-01, -3.1871e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 8.7246e-01,  4.4404e-01, -7.8066e-01,  7.3331e-01],\n",
      "         [ 1.8880e+00, -1.1581e-01,  1.6172e-01,  7.4427e-01],\n",
      "         [ 6.6065e-01,  9.0062e-01, -1.2541e-01, -1.0446e+00],\n",
      "         [ 6.3471e-01,  7.1175e-01, -4.1411e-02,  1.0391e+00]]])\n",
      "tensor([[[0.2294, 0.7706, 0.0000, 0.0000],\n",
      "         [0.7755, 0.2245, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.3686, 0.2401, 0.0706, 0.3207],\n",
      "         [0.6130, 0.0826, 0.1091, 0.1953],\n",
      "         [0.3438, 0.4371, 0.1567, 0.0625],\n",
      "         [0.2447, 0.2643, 0.1244, 0.3666]]])\n"
     ]
    }
   ],
   "source": [
    "# encoder self-attention mask\n",
    "# mask shape: [batch_size, max_src_len, max_src_len], 值为1或-inf\n",
    "\n",
    "# 有效位置矩阵,有word为1,padding的为0\n",
    "vaild_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) for L in src_len]), 2)\n",
    "\n",
    "# 带批信息的两个矩阵相乘\n",
    "# 自己乘自己的转置，可以得到每个位置和别的位置的邻接关系矩阵\n",
    "vaild_encoder_pos_maxrix = torch.bmm(vaild_encoder_pos, vaild_encoder_pos.transpose(1, 2))\n",
    "invaild_encoder_pos_matrix = 1 - vaild_encoder_pos_maxrix\n",
    "\n",
    "# Ture表示需要对这个位置进行mask（padding的没有内容）\n",
    "mask_encoder_self_attention = invaild_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "# demo\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(src_len)\n",
    "print(score)\n",
    "print(masked_score)\n",
    "print(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# intra-attention mask\n",
    "# Q * K^T :[batch_size, tgt_seq_len, src_seq_len]\n",
    "vaild_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0) for L in src_len]), 2)\n",
    "vaild_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len)-L)), 0) for L in tgt_len]), 2)\n",
    "# print(vaild_encoder_pos)\n",
    "# print(vaild_decoder_pos)\n",
    "vaild_cross_pos_matrix = torch.bmm(vaild_decoder_pos, vaild_encoder_pos.transpose(1, 2))\n",
    "invaild_cross_pos_matrix = 1 - vaild_cross_pos_matrix\n",
    "mask_cross_attention = invaild_cross_pos_matrix.to(torch.bool)\n",
    "print(mask_cross_attention)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4a0e993b1d7c23b73f928b0a34114fba5233a538b73c46f7e69aa6023163c42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
